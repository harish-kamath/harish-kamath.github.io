---
layout: distill
title: RQAE - Hierarchical LLM Representations
date: 2024-10-02 11:12:00-0400
description: Redesigning SAEs from first principles
comments: true
categories: interpretability
authors:
  - name: Harish Kamath
    url: "https://www.hkamath.me"
    affiliations:
      name: Rudra AI

bibliography: rqae.bib
toc:
  - name: Introduction
  - name: State of LLM Interpretability
    subsections:
      - name: The Linear Representation Hypothesis
      - name: Sparse Autoencoders
      - name: What's wrong with SAEs?
  - name: RQAE - A New Formulation
    subsections:
      - name: The Polytope Lens & Spline Operators
      - name: Residual Vector Quantization
      - name: Implementation
  - name: Experiments
    subsections:
      - name: Setup
      - name: Model Properties
      - name: Ablations
      - name: Comparison to SAEs
  - name: Algorithms & Evals
    subsections:
      - name: Discovering New Concepts
      - name: Robust Steering
      - name: Concept Detection
  - name: Conclusion
    subsections:
      - name: Limitations
      - name: Future Directions
---

**NOTE**: This page is still a WIP. I am actively adding new experiments and more details, but the outline and core experiments are in place. I expect to finish everything by the 17th.

**TL;DR:** We propose a new architecture to interpret LLM representations, called RQAE (Residual Quantization Autoencoder). RQAE learns features hierarchically, by applying residual vector quantization on the residual stream of an LLM. RQAE has equivalent or better reconstruction than SAEs, with much higher capacity. We show that RQAEs are equivalent to learning meta SAEs end-to-end, and show that they greatly help solve the issue with feature splitting and absorption in SAEs. Additionally, we show how RQAE can be used in three common interpretability tasks that SAEs are used for: finding new features (i.e. dictionary learning a set of features), activation steering, and concept detection.

# Introduction

This section will walk through some existing work in interpretability, and how it leads to RQAE. Even if you're already familiar with most of the recent interpretability work, it's worth skimming, just to get a sense of what problems we are trying to fix with RQAE and why.

## World Models

What's in a LLM representation? Well, we have some idea that it has a world model, because it is *really* good at predicting the next token, and compresses knowledge that it learns incredibly efficiently. If we consider how we humans think about the world, we like to imagine that we have a world model as well. What is a world model? Well, essentially:
1. There are a large number of things that can and will happen in the world
2. We observe what happens, and we reason about what happens with a much, much smaller set of "latent" features.

For example, if we see someone drop a glass bottle, then we expect the bottle to shatter when it hits the ground. This is not because we have observed exactly that person dropping exactly that bottle before. It's because we have learned a very small set of latent features about basic physics, which we can use to model the bottle breaking.

This is exactly why LLMs are so exciting - they just don't have the capacity to store all of the knowledge that we know they have (i.e. to actually memorize the training data), when we ask them things, which means that they must also be working with some set of latent features that they can manipulate to answer the next token prediction task.

## Linear Representations and Superposition

There's a lot of evidence that LLMs describe features as simply directions in space. This is called the Linear Representation Hypothesis. It's very flexible - for example, you can figure out how to define a causal inner product, meaning that vectors which are orthogonal to one another are also causally linked.

What do you do with these directions? Well, you *compose* them, just like we do when we think of any specific thing. For example, we might think of a dog as a composition of the 'animal' feature, and the 'pet' feature, but minus the 'feline' feature. Features also scale differently depending on the subject: the same dog might have more of the 'smart' feature than a goldfish.

<blockquote>
<b>Definition 1:</b> A linear representation has the following two properties:

<ol style="line-height: 1em; font-size: 0.9em; margin: 1.5em 0.5em;">
  <li><b>Composition as Addition</b>: The presence of a feature is represented by adding that feature.</li>
  <li><b>Intensity as Scaling</b>: The intensity of a feature is represented by magnitude.</li>
</ol>
</blockquote>

There is one important difference between us and LLMs, however: We know that LLMs have bottlenecks in their architecture. Consider any layer of the transformer. The *only* way that layer communicates with any other layer, and as a result with the inputs and outputs of the model, is through the residual stream. However, the residual stream is (relatively) very small (maybe O(1k)~O(10k)) in dimensionality $D$. This stream is just a vector (for each token) - this means that it can only have up to $D$ orthogonal vectors, or completely *unique* directions. If we trust the LRH, and we know that the LLM must have *way* more features than that - what gives?

There was a neat idea called superposition which was introduced to answer this. In a nutshell, superposition states that LLMs define features in only *almost*-orthogonal vectors, and are robust to the interference that arises with trying to query a single feature. Since the number of almost-orthogonal vectors grows exponentially with $D$, we're saved!

Okay, let's just assume the LRH is true. What about it? Well, there's the good, and the bad.

The good is that we have tools to deal with directions. Consider the case that the LRH was not true. Then, our next best guess would probably be the Polytope Lens. While this method is exact (and in it's full form, follows exactly the computations made by the model), it's also hopelessly complicated. In contrast, directions are just vectors! We have norms, outer/inner products, affine transformations, etc. to manipulate and understand directions. The LRH actually makes it much simpler for us to understand how an LLM might define and use features.

The bad is that superposition means that features interfere. This means that it may be really hard to separate features that we know should be separated - maybe even impossible. Let's go back to our dog example. Imagine that the model only saw examples in the data about dogs being smart. It would probably be hard, then, for the model to separate the "dog" feature from the "smart" feature - even though we know that in reality, some dog breeds are smarter than others! (This is just a toy example - obviously, real-world LLM training dynamics are probably more complicated than just single correlations between two features)

There's one more thing to notice about the LRH. That is, similar features should correspond to similar directions. For example, the "dog" feature should be closer to the "cat" feature than to the "shark" feature. This means that, when measuring by cosine similarity, we should see clusters forming which correspond to similar features. This makes the issue with superposition even more troubling, because both distinct features and related features will have non-zero cosine similarity.

## Sparse Autoencoders

Regardless of superposition and interference, we would *really* like to use the LRH, since it seems to be much simpler than the alternative, and there's a lot of empirical evidence for it. Here is where sparse autoencoders come in. In a nutshell, SAEs take inspiration from sparse coding theory (you should read Chapter 7 from Wright and Ma if you're interested - it's fascinating to study) and are built on the claim that sparse $\iff$ interpretable. I like the way that Anthropic framed SAEs in their work here:

// Image of SAEs modeling a larger model

Thinking back to our "World Models" section, this actually makes a lot of sense! We can think of the "larger" model as the set of disentangled latent variables that model the real world. Importantly, the larger model only activates sparsely - this means that any interaction only takes in a few features. Sparse autoencoders try to reconstruct this larger model.

<blockquote>
Definition 2: A sparse autoencoder $S$ of size $n$ is a model that takes in an input $r \in \mathbb{R}^d$ and does the following:

$$
S(r) = W_{out}\sigma(W_{in}r  + b_{in}) +b_{out}
$$

where $\sigma$ is some nonlinearity (usually ReLU), and $W_{out} \in \mathbb{R}^{d x n}, W_{in} \in \mathbb{R}^{n x d}$ (for $n \>\> d$). $b_{in} \in \mathbb{R}^n, b_{out} \in \mathbb{R}^d$ are bias terms.

S is trained to recover $r$ with sparsity in $\sigma(W_{in}r)$ - common methods to induce sparsity include TopK or just thresholding. Thus, $\sigma(W_{in}r + b_{in})$ is zero in most dimensions.
</blockquote>

At a high level, you can think of a SAE as conducting the following algorithm.

1. Begin with some entangled representation $r$. $W_{in}$ consists of $n$ vectors, which are "probe" vectors.
2. You compare $r$ against each probe vector by calculating their dot product. Keep in mind that dot product is just the cosine similarity, scaled by some magnitudes.
3. You add a bias to each of these dot products, because some features just scale differently than others depending on how intense they need to be when they do show up.
4. You ReLU, so that you can drop the features which you are *certain* won't show up in $r$. If you didn't ReLU, then your probe would ever only be able to react linearly to features - i.e. every feature would have to proportionally go up or down.
5. The output you have know are $n$ coefficients which we will call $C = c_1, c_2, \dots$. These represent the intensity that you think the feature exists in $r$.
6. You only choose $k$ of the highest intensities. This is to make the model sparse - you are telling the model that it can only take the $k$ features it is most confident in, to reconstruct $r$.
7. Finally, you multiply $C$ by the columns of $W_{out}$, which are the actual feature vectors that you have learned.

The first thing to notice is that this is a great way to model the LRH. Specifically, you have separated the SAE into (1) a part that probes a representation for relevant "features" (it's just one-dimensional, you just project onto the probe), and (2) uses that probing information to actually choose features. Each representation is a linear combination of the features defined in $W_{out}$, just as the LRH describes. 

**NOTE**: If you're paying attention, you should notice that each (probe, feature) pair of vectors are closely linked - the cosine similarity that a representation has to a probe directly defines the intensity of the feature. This is also why, at the beginning of training, the encoder is initialized as the transpose of the decoder so that the probes are exactly the features.

The second thing to notice is that an SAE can only learn a finite number $n$ of features, which is pre-defined before training even starts. This is a problem, because this will make the model fundamentally sensitive to interference. To give a toy example, let's consider there to be two "ground-truth" features $f_1$ and $f_2$ - we know these features to be similar but not exactly the same, for example, "plants" and "animals". Our model has a finite capacity of features it can learn, and let's say the distinction between these two features didn't make the cut. How will the model react?

## The Feature Hierarchy

The answer to the question posed above, essentially, is that: the model will learn an average direction that will fire weakly for both features - for example, a "taxonomy kingdom" feature. In fact, it might learn a *few* features that weakly activate for instances that are more better defined with separate "plant" and "animal" features - for example, it might learn a "living" feature, or an "on Earth" feature. Clearly, we probably want some level of control (or at least some idea) on the specificity of features that SAEs learn.

To be very explicit, we'll define three widely observed issues with interpretability, that all stem from this intuition:

1. **Feature splitting**. If you train a wider SAE, you will notice that these more "general" features split into smaller, more specific features.
2. **Feature absorption**. This is feature splitting *when you don't want it*. Specifically, you have learned separate features in your SAE that are describing the same ground-truth feature - as a result, representations with that ground-truth feature are split across the two learned features without any discernable pattern.
3. **Feature shrinkage**. This is the phenomenon by which SAEs routinely *underestimate* the intensity of a given feature. It happens because of the sparsity penalty during training - there's a clear example given here. JumpReLU's largely mitigate this issue, but it's related to the first two because it happens due to learning entangled features (specifically, the model will underestimate a feature's intensity because it wants to account for other features that will interfere), so we keep it in here for reference.

By now, you've probably realized that the *core* reason these issues exist, is because we haven't even defined what a feature actually is, and how "atomic" a feature can actually get! And that's for a reason - I have no clue.

There's a cool paper from UChicago that attempts to answer this question (by the same authors that wrote the paper about causal inner products). They split up features in two dimensions: **hierarchical** features, such as organism -> (plant -> (tree, bush, etc.), animal -> (bird, reptile, fish, etc.)), and **categorical** features, such as (dog, cat, hamster, etc.). Hierarchical features are organized orthogonal to each other, while categorical features organize themselves into polytopes. It's not clear how *complete* this definition of features is - but we'll use this definition moving forwards, since adding hierarchy *does* give us a sense for atomic features.

SAEs learn features as if they are all categorical. It *can* learn hierarchical features (for example, it can learn separate features for organism, plant, animal, etc.) - but there is nothing in the architecture that *encourages* it to learn such features. If I had to guess, I would assume that it learns hierarchy based on the frequency of the concepts in the training data alone.

## Brainstorming a New Architecture

Let's say we had a new architecture that did learn features hierarchically. Specifically, instead of learning features individually, it will assign features to a hierarchy, with parent and child features, such that any time a feature is activated, it's parent will also be activated, and zero or one of it's children will be activated. Then, we would have a tangible way of addressing the three issues presented above:

1. If a feature splits, then we define the base feature as higher in the hierarchy, and the split features as lower in the hierarchy.
2. Feature absorption is a result of going deeper in the hierarchy than you want to. If you notice two features that should be absorbed, you should ignore them and only consider their parent feature.
3. Feature shrinkage should be mitigated, because features at a given layer in the hierarchy will mostly "compete" with other features on the same level. That is, the intensity of a parent feature shouldn't depend on the intensity of it's children, and vice versa.

# RQAE

Finally, we will introduce the proposed architecture, RQAE (Residual Quantization Autoencoder). The purpose of this architecture is to learn features hierarchically. It is still a dictionary learning method, but it uses quantization instead of MLPs to learn the dictionary.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/rqae.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 1.</b> An overview of RQAE. Note that this diagram does not include biases, which are present at each linear layer.
</div>

The core idea behind RQAE, is to use residual vector quantization (RVQ) to autoencode the LLM representation. If you aren't familiar with RVQ, here's a great explanation. We specifically use a variant of FSQ, that uses hyperspheres instead of hypercubes to define codebooks. Otherwise, there's only one main difference between regular RVQ and RQAE - we learn a separate linear in/out layer, per quantization layer.

## A Close Resemblance to SAEs

It's hard to propose a new architecture, when the existing architecture has so much strong empirical evidence backing it (for example, Gemmascope used O(training compute of GPT-3) to train all SAEs). Luckily, we can show that there's a very close connection between RQAEs and SAEs.

<blockquote>
<b>Claim:</b> A single layer of RQAE can be equivalently defined as an SAE.
<br />
<b>Proof:</b>
</blockquote>

You'll notice from **Figure 1** that there's one major difference from what we defined above: RQAE projects into a very small subspace - in this case, a subspace of dimension 4! Why would we do that? Well, there's two main reasons:

1. Quantization works much better with lower-dimension codebooks. This was shown clearly in the FSQ paper. With higher dimensions, it's hard to ensure that codebooks are being utilized well throughout training (similar to dead latents that show up in SAEs).
2. There's a lot of reason to believe that features (even feature manifolds) are very low-rank in LLMs. The entire body of work surrounding LoRA and PEFT methods suggest that LLM behavior can be controlled in very small subspaces. As a result, it's likely that we aren't *dropping* features, because individual features or feature manifolds aren't likely to span a large amount of dimensions.

As a result, you can think of a single layer of RQAE as a really *shitty* SAE.

## RQAE $\approx$ Meta-SAE

There's a great work done recently from MATS, that learns a SAE on top of SAEs. Specifically, it learns a SAE on the feature vectors from another SAE, and shows that this meta SAE does a good job at properly splitting features. This should make sense to you - recall that SAEs are built to have some amount of small interference between features, so a meta SAE tries to learn new "features" that disentangle that interference further. However, it certainly *feels* a little hacky to learn this meta SAE post training. For example - what if the SAE knew that this "meta SAE" would split it's features further? Would it learn different, less specific features?


Well - there's actually great news! We can prove that RQAE simplifies to a special case of meta SAEs architecturally - except RQAEs are trained end-to-end. The core insight is that quantization $\approx$ SAEs anyways:

Then, it's relatively simple to show that 2 layers of RQAE $\approx$ a meta SAE:

<blockquote>
<b>Claim:</b> Two layers of RQAE can be equivalently defined as a meta SAE.
<br />
<b>Proof:</b>
</blockquote>

## Design Decisions

At this point, you should be fairly convinced that a RQAE is very similar to a SAE. Here, we'll describe specific design decisions we made, that make them different. Here's the default model we will reference:

| Parameter | Value | Description |
|-----------|-------|-------------|
| LLM | Gemma 2 2B | Base model to interpret |
| Layer | Residuals after the center layer (layer 12 for Gemma 2 2B) | What layer of activations to train on |
| Training Data | FineWeb-Edu | Dataset used to get activations to train on |
| num_tokens | 1B | How many tokens to train on |
| context_length | 128 | Context length to train on |
| num_quantizers | 1024 | Number of residual quantizer layers |
| codebook_dim | 4 | Dimension of codebook |
| codebook_size_per_dim | 5 | Size of codebook, per codebook dimension$^\star$ |

<div class="caption">
    <b>Table 1.</b> Default parameter values for RQAE.
</div>

**FSQ:** We use a variant of FSQ, that uses hyperspheres instead of hypercubes to define codebooks. Quantization doesn't have a clear equivalent of "intensity", compared to SAEs. Thus, in contrast to SAEs, we allow the decoder features to have unlimited weight norm.

**Normalize Activations:** Before passing the LLM activations into RQAE, we normalize them. We just use the final RMS norm layer of the LLM, since we are only working on the residual stream anyways. (This is *different* though than other work, since we actually are using the sample-wise norm)

**FineWeb-Edu:**

## Properties

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/cosine_similarity.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/mags.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> MSE Loss and Cross Entropy Loss Difference for RQAE trained on Gemma 9B and 2B models.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/codebook_usage.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/codebook_values.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> Graph of distribution of codebooks used, as well as how this translates to features being sparse.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/sae_feature_codebook_viz.png class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/random_feature_codebook_viz.png class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> Graph of distribution of codebooks used, as well as how this translates to features being sparse.
</div>

## Reconstruction

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/mse_ce_comparison.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> MSE Loss and Cross Entropy Loss Difference for RQAE trained on Gemma 9B and 2B models.
</div>

// Graph of Gemma 2B, Gemma 9B, and SAEs vs RQAEs reconstruction loss performance. X axis is L0, Y axis is CE Loss. 2 lines, 1 for each SAE. Then, there are horizontal dotted lines for each # quantizers.

## Interpretability

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/feature_clusters_012.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> Three examples of clusters, starting at layer 0 and splitting on layer 1.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/features_cluster_interference.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2b.</b> Different codebooks at layer 1 can interfere with each other.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/features_smooth.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2c.</b> Other directions on the codebook_dim unit sphere can be used to interpret the same feature.
</div>

# Algorithms

// Explanation of how SAEs are evaluated on a given dataset. As a result, found features depend a lot on the dataset.

## Feature Distributions

// Explanation of how to use vMF for distributions per layer

// Explanation of how this is related to cosine sim

## Setup

// Description of monology

// Description of models used

// Description of metrics tracked (cosine sim, correlated dimensions)

## Finding New Features

// Example, walked through

```python


```

// 3 best examples

### Feature Splitting

// Explanation of finding split features (iteratively applying algorithm)

// 1 example of feature splitting

## Activation Steering

// Pseudocode for steering

// 3 best examples

### Steering with multiple features

// Psuedocode for steering with multiple features

// 1 example of steering with multiple features

## Toxicity Detection

// 3 best examples

# Ablations

# Towards a more General Theory

# Conclusion

