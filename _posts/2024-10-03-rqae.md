---
layout: distill
title: RQAE - Hierarchical LLM Representations
date: 2024-10-16 11:12:00-0400
description: Designing a hierarchical SAE
comments: true
categories: interpretability
authors:
  - name: Harish Kamath
    url: "https://www.hkamath.me"
    affiliations:
      name: Rudra AI
bibliography: rqae.bib
toc:
  - name: Introduction
    subsections:
      - name: World Models
      - name: Linear Representations and Superposition
      - name: Sparse Autoencoders
      - name: The Feature Hierarchy
      - name: Brainstorming a New Architecture
  - name: RQAE
    subsections:
      - name: A Close Resemblance to SAEs
      - name: RQAE â‰ˆ Meta-SAE
      - name: Design Decisions
      - name: Properties
      - name: Reconstruction
      - name: Interpretability
  - name: Algorithms
    subsections:
      - name: Feature Distributions
      - name: Setup
      - name: Finding New Features
      - name: Activation Steering
      - name: Toxicity Detection
  - name: Ablations
  - name: Towards a more General Theory
  - name: Conclusion
---

*NOTE: This page is still a WIP. I am actively adding new experiments and more details, but the outline and core experiments are in place. I expect to finish everything by the 17th.*

**TL;DR:** We propose a new architecture to interpret LLM representations, called RQAE (Residual Quantization Autoencoder). RQAE learns features hierarchically, by applying residual vector quantization on the residual stream of an LLM. RQAE has equivalent or better reconstruction than SAEs, with much higher capacity. We show that RQAEs are equivalent to learning meta SAEs end-to-end, and show that they greatly help solve the issue with feature splitting and absorption in SAEs. Additionally, we show how RQAE can be used in three common interpretability tasks that SAEs are used for: finding new features (i.e. dictionary learning a set of features), activation steering, and concept detection.

# Introduction

The purpose of interpretability is to decompose a deep model into human-understandable features. This has led to some incredibly interesting work: my favorites include visualizing what parts of an image a model uses to predict a class <d-cite key="gradcam" /> and seeing how RL agents identify friends and enemies in a game <d-cite key="coinrun" />.

However, Large Language Models (LLMs) may be more complicated to interpret. Text is a much denser medium than visuals when it comes to interpretable features, and LLMs are orders of magnitude larger than any other models we've ever trained. This work is heavily inspired by [transformer-circuits](https://transformer-circuits.pub/), which has laid out a foundation for how we can begin to interpret LLMs. If you're new to interpretability, I would recommend reading these works first, but I'll provide a quick rundown of the core concepts in this section.

Other work has explored how different layers of the LLM contribute to different functions (for example, the feedforward layers store facts that the LLM knows <d-cite key="rome" />). However, the scope of this work is limited to considering the residual stream of a LLM. In the future, we hope to extend this model to all other layers of the LLM, as has been done with SAEs <d-cite key="gemmascope" />.

## World Models

What is a human interpretable feature, and why would they exist in LLMs? Well, we know that LLMs store world models because they are *really* good at predicting the next token, and compressing the training data to the extent that they do requires some latent understanding of the world. Ha & Schmidhuber define this more rigorously <d-cite key="worldmodels" />, but roughly:
1. There are a large number of things that can and will happen in the world
2. We observe what happens, and we reason about what happens with a much, much smaller set of "latent" features.

For example, if we see someone drop a glass bottle, then we expect the bottle to shatter when it hits the ground. This is not because we have observed exactly that person dropping exactly that bottle before. It's because we have learned a very small set of latent features about physics, which we can use to model the bottle breaking.

This is exactly why LLMs are so exciting - they just don't have the capacity to store all of the knowledge that we know they have (i.e. to actually memorize the training data), which means that they must also be working with some set of latent features that they can manipulate to solve the next token prediction task!

## Linear Representations and Superposition

How do LLMs organize and use these latent features? There's a lot of evidence that LLMs describe features as simply directions in space, known as the Linear Representation Hypothesis (LRH). If true, it's a very powerful framework - for example, you can define a causal inner product, and show that vectors which are orthogonal to one another are also causally linked <d-cite key="lrhgeom" />.

Then, a full LLM activation is the sum of multiple atomic interpretable features. For example, we might think of a dog as the 'animal' feature plus the 'pet' feature, minus the 'feline' feature. Features also scale differently depending on the subject: a dog will have more of the 'smart' feature than a goldfish.

We formalize the LRH with the definition given [here](https://transformer-circuits.pub/2024/july-update/index.html#linear-representations-mathematical):
<blockquote>
<b>Definition 1:</b> A linear representation has the following two properties:

<ol style="line-height: 1em; font-size: 0.9em; margin: 1.5em 0.5em;">
  <li><b>Composition as Addition</b>: The presence of a feature is represented by adding that feature.</li>
  <li><b>Intensity as Scaling</b>: The intensity of a feature is represented by magnitude.</li>
</ol>
</blockquote>

There is one problem with the LRH: the residual stream of an LLM with width $d$ lies in the space $\mathbb{R}^d$. However, there can be at most $d$ orthogonal vectors in this space (any basis you choose), which means that there can be at most $d$ completely unique "feature directions".

In order for the LRH to be true, models must learn features in [superposition](https://transformer-circuits.pub/2022/toy_model/index.html). This means that features *will* interfere with each other, and you can not fully separate feature interactions. However, you can also fit exponentially more features in a $d$-dimensional space which are only $\epsilon$-orthogonal to each other!

There's one more thing to notice about the LRH: similar features should correspond to similar directions. For example, the "dog" feature should be closer to the "cat" feature than to the "shark" feature. Thus, when measuring by cosine similarity, we should see clusters forming which correspond to similar features.

## Sparse Autoencoders

Sparse Autoencoders (SAEs) try to take advantage of the LRH by learning an overcomplete basis to take features out of superposition <d-cite key="sae" />. SAEs take inspiration from sparse coding theory (Chapter 7 from [Wright and Ma](https://book-wright-ma.github.io/) is a great introduction): it suggests that learning a dictionary of features that only fire sparsely (i.e. a very small number of features are required to define each activation) also results in that dictionary being human-interpretable.

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/disentangled.png class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    A figure taken from <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a>. In this case, the observed model is our LLM, and the disentangled model is what we are trying to learn with a SAE.
</div>

Looking back at the [World Models](#world-models) section, this actually makes a lot of sense! We can think of the "larger" model as the set of disentangled latent variables that model the real world. Importantly, the larger model only activates sparsely - this means that any interaction only takes in a few features. Sparse autoencoders try to reconstruct this larger model, in order to move out of superposition.

<blockquote>
<b>Definition 2:</b> A sparse autoencoder $S$ of size $n$ is a model that takes in an activation $r \in \mathbb{R}^d$ and does the following:

$$
C(r) = \sigma(W_{in}r  + b_{in})
$$
$$
S(r) = W_{out}C(r) +b_{out}
$$

where $\sigma$ is some nonlinearity (usually ReLU), and $W_{out} \in \mathbb{R}^{d \times n}, W_{in} \in \mathbb{R}^{n \times d}$ (for $n \gg d$). $b_{in} \in \mathbb{R}^n, b_{out} \in \mathbb{R}^d$ are bias terms.

S is trained to recover $r$ while inducing sparsity in $C(r)$, meaning that $C(r)$ is zero in most dimensions - common methods to induce sparsity include TopK, L1 loss, or just thresholding.
</blockquote>

To explicitly draw the connection to the LRH, a SAE performs the following algorithm:

1. Begin with some entangled LLM activation $r$, and consider that $W_{in}$ consists of $n$ vectors, which we will call "probe" vectors.
2. Compare $r$ against each probe vector by calculating their dot product.
3. Add a bias to each of these dot products, because some features scale differently than others depending on their average intensities.
4. Apply a nonlinearity (usually ReLU), to act as a filter for sparsity (by dropping probes that are very unlikely to zero).
5. There are now $n$ coefficients  $C(r) = \[c_1, c_2, ...\]$, which are the intensities of each feature in $r$.
7. Multiply $C$ by the columns of $W_{out}$, which are the actual feature vectors (directions) that you have learned.

*NOTE: Notice that each (probe, feature) pair of vectors are closely linked - the cosine similarity ($\propto$ dot product) that a representation has to a probe directly defines the intensity of the feature. At the beginning of training, the encoder is initialized as the transpose of the decoder so that probes = features.*

However, an SAE can only learn a set number $n$ of features, and it's likely that $n \ll N$ for the true number of features $N$ the LLM uses. To give a toy example, let's consider there to be two "ground-truth" features $f_1$ and $f_2$ - these features are similar but not exactly the same (e.g. different breeds of dogs). How will the SAE learn these features?

## The Feature Hierarchy

The answer to the question posed above is: the model will learn an average direction that will fire weakly for these features - for example, a "dog" feature. It might also learn a few features that weakly activate for related, more general features - for example, a "living" feature, or a "pet" feature. Clearly, we want some level of control on the specificity of features that SAEs learn.

There are three widely observed issues with SAEs, that all stem from this intuition:

1. **Feature splitting**. If you train a wider SAE, you will notice that more general features split into smaller, more specific features.
2. **Feature absorption**. You learn two separate features in your SAE that are describing the same ground-truth feature - as a result, representations with that ground-truth feature are split across the two learned features without any discernable pattern (i.e. the difference between the two features is spurious).
3. **Feature shrinkage**. SAEs routinely *underestimate* the intensity of a given feature. It happens because of the sparsity penalty during training - see [this work](https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes) for a clear example and more. JumpReLUs <d-cite key="jumprelu" /> largely mitigate this issue, but it's related to the first two because it happens due to learning entangled features (an SAE will underestimate a feature's intensity because it wants to account for other features that will interfere).

All three of these issues stem from the same underlying cause: SAE features are not necessarily atomic. They might need to be broken down or grouped together, but it's difficult to tell which is which, and the training objective doesn't bias the model one way or the other. This begs the question: what is an atomic feature?

A paper from UChicago attempts to answer this question <d-cite key="cathierarchy" />. They split up features in two dimensions: **hierarchical** features, such as organism -> (plant -> (tree, bush, etc.), animal -> (bird, reptile, fish, etc.)), and **categorical** features, such as (dog, cat, hamster, etc.). Hierarchical features are organized orthogonally to each other, while categorical features organize themselves into polytopes. It's not clear how *complete* this definition of features is - but we'll use this definition moving forwards, since adding hierarchy *does* define atomic features.

SAEs treats features as if they are all categorical. It *can* learn hierarchies of features (for example, it can learn separate features for organism, plant, animal, etc.) - but there is nothing in the architecture that *encourages* it to learn such features. My best guess is that it learns hierarchy based on the frequency of the concepts in the training data alone, since [this work](https://transformer-circuits.pub/2024/september-update/index.html#oversampling) finds that SAEs learn more granular features when trained on different datasets. However, this is still an open question.

## Brainstorming a New Architecture

Consider a new architecture that did learn features hierarchically. What would that look like? It should assigns features to a hierarchy, with parent and child features, such that any time a feature is activated, it's parent will also be activated, and zero or one of it's children will be activated. Then, we could address the three issues presented in the previous section:

1. If a feature splits, then we define the base feature as higher in the hierarchy, and the split features as lower in the hierarchy.
2. Feature absorption is a result of going deeper in the hierarchy than you want to. If you notice two features that should be absorbed, you should ignore them and only consider their parent feature.
3. Feature shrinkage should not happen, because features at a given layer in the hierarchy will only "compete" with other features on the same level. That is, the intensity of a parent feature shouldn't depend on the intensity of it's children, and vice versa. Since exactly one feature will be activated at each layer of the hierarchy, there is no incentive to account for interference.

# RQAE

Finally, we introduce the proposed architecture, RQAE (Residual Quantization Autoencoder). This architecture follows the model described in the previous section, and learns features hierarchically. Check out [Figure 1](#fig1) for an illustrated version.

The core idea behind RQAE is to use residual vector quantization (RVQ) to autoencode the LLM representation. If you aren't familiar with RVQ, [here](https://drscotthawley.github.io/blog/posts/2023-06-12-RVQ.html) is a great explanation. The primary difference is that we inject a linear in/out layer between each quantization step, which allows the model to iteratively choose different subspaces of the representation space to quantize.

We use a quantization variant of FSQ <d-cite key="fsq" />, that uses hyperspheres instead of hypercubes to define codebooks. There are two benefits of using FSQ:
1. There are no issues with codebook collapse. Most codebooks are utilized fully, which is especially important when we have many layers.
2. It restricts the geometry of subspaces in the representation space to ellipsoid objects only (hypersphere + affine transform). Since ellipsoids can approximate convex polytopes well <d-cite key="ellipsoid" />, subspaces are encouraged to take the shape of the categorical features mentioned above <d-cite key="cathierarchy" />.

<div class="row pt-3" id="fig1">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/rqae.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 1.</b> An overview of RQAE. This diagram does not include biases, which are present at each linear layer.
</div>

RQAE fits the definition from [above](#brainstorming-a-new-architecture) well. For any given LLM activation, every layer returns exactly one codebook. Later layers are dependent on earlier layers (not exactly, as we will see in [Algorithms](#algorithms)), enforcing a hierarchy between layers. We define the learned features to be the entries of the codebooks after they are projected by the linear out layer. Thus, a RQAE model with $n$ layers and $c$ codebooks per layer learns $c \times n$ unique features, but can represent $c^n$ different activations.

## A Close Resemblance to SAEs

It's hard to propose a new architecture, when the existing architecture already has a large body of work and plenty of empirical evidence (for example, Gemmascope <d-cite key="gemmascope" /> used $O($training compute of GPT-3$)$ to train all SAEs). Thus, we draw a connection between the layers of a RQAE and a SAE.

Conceptually, RQAE has the same parts as the SAE algorithm mentioned [above](#sparse-autoencoders): linear in layers are "probes", intensity coefficients are constrained to lie on the hypersphere, and features are projected codebooks. However, we can go a step further and show mathematically that these two are the same:

<blockquote>
<b>Lemma:</b> A single layer of RQAE can be equivalently defined as an SAE.
<br />
<b>Proof:</b> Still a WIP. Will follow <a href="https://euclaise.xyz/vq-is-mlp/">this work</a> closely
</blockquote>

In practice, notice that in [Figure 1](#fig1) we use a much smaller codebook dimension. This is needed for quantization (especially FSQ, whose codebook size grows exponentially with respect to codebook dimension). However, this is not a concern, because there is a large body of work that suggests LLM features and feature manifolds are represented in very low dimensional subspaces (i.e. rank deficiency)<d-cite key="lora" />. 

## RQAE $\approx$ Meta-SAE



There's a great work done recently from MATS, that learns a SAE on top of SAEs. Specifically, it learns a SAE on the feature vectors from another SAE, and shows that this meta SAE does a good job at properly splitting features. This should make sense to you - recall that SAEs are built to have some amount of small interference between features, so a meta SAE tries to learn new "features" that disentangle that interference further. However, it certainly *feels* a little hacky to learn this meta SAE post training. For example - what if the SAE knew that this "meta SAE" would split it's features further? Would it learn different, less specific features?


Well - there's actually great news! We can prove that RQAE simplifies to a special case of meta SAEs architecturally - except RQAEs are trained end-to-end. The core insight is that quantization $\approx$ SAEs anyways:

Then, it's relatively simple to show that 2 layers of RQAE $\approx$ a meta SAE:

<blockquote>
<b>Claim:</b> Two layers of RQAE can be equivalently defined as a meta SAE.
<br />
<b>Proof:</b> Still a WIP. General idea is that meta SAE = SAE on residuals of what SAE missed.
</blockquote>

## Design Decisions

At this point, you should be fairly convinced that a RQAE is very similar to a SAE. Here, we'll describe specific design decisions we made, that make them different. Here's the default model we will reference:

| Parameter | Value | Description |
|-----------|-------|-------------|
| LLM | Gemma 2 2B | Base model to interpret |
| Layer | Residuals after the center layer (layer 12 for Gemma 2 2B) | What layer of activations to train on |
| Training Data | FineWeb-Edu | Dataset used to get activations to train on |
| num_tokens | 1B | How many tokens to train on |
| context_length | 128 | Context length to train on |
| num_quantizers | 1024 | Number of residual quantizer layers |
| codebook_dim | 4 | Dimension of codebook |
| codebook_size_per_dim | 5 | Size of codebook, per codebook dimension$^\star$ |

<div class="caption">
    <b>Table 1.</b> Default parameter values for RQAE.
</div>

**FSQ:** We use a variant of FSQ, that uses hyperspheres instead of hypercubes to define codebooks. Quantization doesn't have a clear equivalent of "intensity", compared to SAEs. Thus, in contrast to SAEs, we allow the decoder features to have unlimited weight norm.

**Normalize Activations:** Before passing the LLM activations into RQAE, we normalize them. We just use the final RMS norm layer of the LLM, since we are only working on the residual stream anyways. (This is *different* though than other work, since we actually are using the sample-wise norm)

**FineWeb-Edu:**

## Properties

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/cosine_similarity.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/mags.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> MSE Loss and Cross Entropy Loss Difference for RQAE trained on Gemma 9B and 2B models.
</div>

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/codebook_usage.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/codebook_values.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> Graph of distribution of codebooks used, as well as how this translates to features being sparse.
</div>

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/sae_feature_codebook_viz.png class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/random_feature_codebook_viz.png class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> Graph of distribution of codebooks used, as well as how this translates to features being sparse.
</div>

## Reconstruction

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/mse_ce_comparison.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> MSE Loss and Cross Entropy Loss Difference for RQAE trained on Gemma 9B and 2B models.
</div>

// Graph of Gemma 2B, Gemma 9B, and SAEs vs RQAEs reconstruction loss performance. X axis is L0, Y axis is CE Loss. 2 lines, 1 for each SAE. Then, there are horizontal dotted lines for each # quantizers.

## Interpretability

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/feature_clusters_012.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2a.</b> Three examples of clusters, starting at layer 0 and splitting on layer 1.
</div>

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/features_cluster_interference.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2b.</b> Different codebooks at layer 1 can interfere with each other.
</div>

<div class="row pt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% responsive_image path: assets/img/rqae/features_smooth.jpg class: "img-fluid rounded z-depth-1" zoomable: true %}
    </div>
</div>
<div class="caption">
    <b>Figure 2c.</b> Other directions on the codebook_dim unit sphere can be used to interpret the same feature.
</div>

# Algorithms

// Explanation of how SAEs are evaluated on a given dataset. As a result, found features depend a lot on the dataset.

## Feature Distributions

// Explanation of how to use vMF for distributions per layer

// Explanation of how this is related to cosine sim

## Setup

// Description of monology

// Description of models used

// Description of metrics tracked (cosine sim, correlated dimensions)

## Finding New Features

// Example, walked through

```python


```

// 3 best examples

### Feature Splitting

// Explanation of finding split features (iteratively applying algorithm)

// 1 example of feature splitting

## Activation Steering

// Pseudocode for steering

// 3 best examples

### Steering with multiple features

// Psuedocode for steering with multiple features

// 1 example of steering with multiple features

## Toxicity Detection

// 3 best examples

# Ablations

# Towards a more General Theory

# Conclusion

