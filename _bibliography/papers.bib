---
---

@article{DirectInversion,
  title    = {Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models},
  author   = {Elarabawy, A. and Kamath, H. and Denton, S.},
  abstract = {With the rise of large, publicly-available text-to-image diffusion models, text-guided real image editing has garnered much research attention recently. Existing methods tend to either rely on some form of per-instance or per-task fine-tuning and optimization, require multiple novel views, or they inherently entangle preservation of real image identity, semantic coherence, and faithfulness to text guidance. In this paper, we propose an optimization-free and zero fine-tuning framework that applies complex and non-rigid edits to a single real image via a text prompt, avoiding all the pitfalls described above. Using widely-available generic pre-trained text-to-image diffusion models, we demonstrate the ability to modulate pose, scene, background, style, color, and even racial identity in an extremely flexible manner through a single target text detailing the desired edit. Furthermore, our method, which we name Direct Inversion, proposes multiple intuitively configurable hyperparameters to allow for a wide range of types and extents of real image edits. We prove our method's efficacy in producing high-quality, diverse, semantically coherent, and faithful real image edits through applying it on a variety of inputs for a multitude of tasks. We also formalize our method in well-established theory, detail future experiments for further improvement, and compare against state-of-the-art attempts.},
  month    = {November},
  year     = {2022},
  url      = {https://arxiv.org/abs/2211.07825},
  pdf      = {https://arxiv.org/pdf/2211.07825},
  selected = {true}
}


@article{BridgingWorlds,
  abbr     = {ICML Workshop},
  title    = {Bridging Worlds in Reinforcement Learning with Model-Advantage},
  author   = {Modhe, N. and Kamath, H. and Kalyan, A. and Batra, D.},
  abstract = {Despite the breakthroughs achieved by Reinforcement Learning (RL) in recent years, RL agents often fail to perform well in unseen environments. This inability to generalize to new environments prevents their deployment in the real world. To help measure this gap in performance, we introduce model-advantage - a quantity similar to the well-known (policy) advantage function. First, we show relationships between the proposed modeladvantage and generalization in RL â€” using which we provide guarantees on the gap in performance of an agent in new environments. Further, we conduct toy experiments to show that even a sub-optimal policy (learnt with minimal interactions with the target environment) can help predict if a training environment (say, a simulator) helps learn policies that generalize. We then show connections with Model Based RL.},
  month    = {July},
  year     = {2020},
  url      = {https://openreview.net/forum?id=XBRYX4c_xFQ},
  pdf      = {https://openreview.net/pdf?id=XBRYX4c_xFQ},
  selected = {true}
}